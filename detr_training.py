# -*- coding: utf-8 -*-
"""DETR_training.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/122mpFRY6a1yUJqhL3WlKi_FpitVdroDb
"""

!pip install -q git+https://github.com/huggingface/transformers.git timm

"""We also install PyTorch Lightning, as this will be used to train the model."""

!pip install -q pytorch-lightning

from google.colab import drive
drive.mount('/content/drive')

import torch
import json
from pathlib import Path
from PIL import Image

class ade20ksemantic(torch.utils.data.Dataset):
    def __init__(self, img_folder, ann_folder, ann_file, feature_extractor):
        with open(ann_file, 'r') as f:
            self.ade20k = json.load(f)

        # sort 'images' field so that they are aligned with 'annotations'
        # i.e., in alphabetical order
        self.ade20k['images'] = sorted(self.ade20k['images'], key=lambda x: x['id'])
        # sanity check
        if "annotations" in self.ade20k:
            for img, ann in zip(self.ade20k['images'], self.ade20k['annotations']):
                assert img['file_name'][:-4] == ann['file_name'][:-4]

        self.img_folder = img_folder
        self.ann_folder = Path(ann_folder)
        self.ann_file = ann_file
        self.feature_extractor = feature_extractor

    def __getitem__(self, idx):
        ann_info = self.ade20k['annotations'][idx] if "annotations" in self.ade20k else self.ade20k['images'][idx]
        img_path = Path(self.img_folder) / ann_info['file_name'].replace('.png', '.jpg')

        img = Image.open(img_path).convert('RGB')

        # preprocess image and target (converting target to DETR format, resizing + normalization of both image and target)
        encoding = self.feature_extractor(images=img, annotations=ann_info, masks_path=self.ann_folder, return_tensors="pt")
        pixel_values = encoding["pixel_values"].squeeze() # remove batch dimension
        target = encoding["labels"][0] # remove batch dimension

        return pixel_values, target

    def __len__(self):
        return len(self.ade20k['images'])

from transformers import DetrFeatureExtractor
import numpy as np


feature_extractor = DetrFeatureExtractor.from_pretrained("facebook/detr-resnet-50-semantic", size=500, max_size=600)

dataset = ade20ksemantic(img_folder='/content/drive/MyDrive/DETR/ade20k data/val2017',
                             ann_folder='/content/drive/MyDrive/DETR/ade20k data/annotations',
                             ann_file='/content/drive/MyDrive/DETR/ade20k data/annotations/ade20k.json',
                             feature_extractor=feature_extractor)

np.random.seed(42)
indices = np.random.randint(low=0, high=len(dataset), size=50)
train_dataset = torch.utils.data.Subset(dataset, indices[:40])
val_dataset = torch.utils.data.Subset(dataset, indices[40:])

pixel_values, target = train_dataset[2]
print(pixel_values.shape)
print(target.keys())

print("Number of training examples:", len(train_dataset))
print("Number of validation examples:", len(val_dataset))

from torch.utils.data import DataLoader

def collate_fn(batch):
  pixel_values = [item[0] for item in batch]
  encoded_input = feature_extractor.pad_and_create_pixel_mask(pixel_values, return_tensors="pt")
  labels = [item[1] for item in batch]
  batch = {}
  batch['pixel_values'] = encoded_input['pixel_values']
  batch['pixel_mask'] = encoded_input['pixel_mask']
  batch['labels'] = labels
  return batch

train_dataloader = DataLoader(train_dataset, collate_fn=collate_fn, batch_size=3, shuffle=True)
val_dataloader = DataLoader(val_dataset, collate_fn=collate_fn, batch_size=1)

"""Model Training"""

import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Conv2D, MaxPool2D, Dense, Dropout, Flatten
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping

def get_new_model(input_shape):
  '''
  This function returns a compiled CNN with specifications given above.
  '''

  #Defining the architecture of the CNN
  input_layer = Input(shape=input_shape, name='input')
  h = Conv2D(filters=16, kernel_size=(3,3),
             activation='relu', padding='same', name='conv2d_1')(input_layer)
  h = Conv2D(filters=16, kernel_size=(3,3),
             activation='relu', padding='same', name='conv2d_2')(h)

  h = MaxPool2D(pool_size=(2,2), name='pool_1')(h)

  h = Conv2D(filters=16, kernel_size=(3,3),
             activation='relu', padding='same', name='conv2d_3')(h)
  h = Conv2D(filters=16, kernel_size=(3,3),
             activation='relu', padding='same', name='conv2d_4')(h)

  h = MaxPool2D(pool_size=(2,2), name='pool_2')(h)


  h = Dense(64, activation='relu', name='dense_1')(h)
  h = Dropout(0.5, name='dropout_1')(h)
  h = Flatten(name='flatten_1')(h)
  output_layer = Dense(10, activation='softmax', name='dense_2')(h)

  #To generate the model, we pass the input layer and the output layer
  model = Model(inputs=input_layer, outputs=output_layer, name='model_CNN')

  #Next we apply the compile method
  model.compile(optimizer='adam',
                loss='categorical_crossentropy',
                metrics=['accuracy'])


  return model

# Define input shape
input_shape = (28, 28, 1)  # Example input shape, adjust according to your data

# Create the model
model = get_new_model(input_shape)

# Print the model architecture
model.summary()

import pytorch_lightning as pl
import torch

class Detrsemantic(pl.LightningModule):

     def __init__(self, model, lr, lr_backbone, weight_decay):
         super().__init__()

         self.model = model

         self.lr = lr
         self.lr_backbone = lr_backbone
         self.weight_decay = weight_decay

     def forward(self, pixel_values, pixel_mask):
       outputs = self.model(pixel_values=pixel_values, pixel_mask=pixel_mask)

       return outputs

     def common_step(self, batch, batch_idx):
       pixel_values = batch["pixel_values"]
       pixel_mask = batch["pixel_mask"]
       labels = [{k: v.to(self.device) for k, v in t.items()} for t in batch["labels"]]

       outputs = self.model(pixel_values=pixel_values, pixel_mask=pixel_mask, labels=labels)

       loss = outputs.loss
       loss_dict = outputs.loss_dict

       return loss, loss_dict

     def training_step(self, batch, batch_idx):
        loss, loss_dict = self.common_step(batch, batch_idx)
        # logs metrics for each training_step,
        # and the average across the epoch
        self.log("training_loss", loss)
        for k,v in loss_dict.items():
          self.log("train_" + k, v.item())

        return loss

     def validation_step(self, batch, batch_idx):
        loss, loss_dict = self.common_step(batch, batch_idx)
        self.log("validation_loss", loss)
        for k,v in loss_dict.items():
          self.log("validation_" + k, v.item())

        return loss

     def configure_optimizers(self):
        param_dicts = [
              {"params": [p for n, p in self.named_parameters() if "backbone" not in n and p.requires_grad]},
              {
                  "params": [p for n, p in self.named_parameters() if "backbone" in n and p.requires_grad],
                  "lr": self.lr_backbone,
              },
        ]
        optimizer = torch.optim.AdamW(param_dicts, lr=self.lr,
                                  weight_decay=self.weight_decay)

        return optimizer

     def train_dataloader(self):
        return train_dataloader

     def val_dataloader(self):
        return val_dataloader

from transformers import DetrConfig, DetrForSegmentation

model = DetrForSegmentation.from_pretrained("facebook/detr-resnet-50-semantic")
state_dict = model.state_dict()
# Remove class weights
del state_dict["detr.class_labels_classifier.weight"]
del state_dict["detr.class_labels_classifier.bias"]
# define new model with custom class classifier
config = DetrConfig.from_pretrained("facebook/detr-resnet-50-semantic", num_labels=250)
model.load_state_dict(state_dict, strict=False)

model = Detrsemantic(model=model, lr=1e-4, lr_backbone=1e-5, weight_decay=1e-4)

# pick the first training batch
batch = next(iter(train_dataloader))
# forward through the model
outputs = model(pixel_values=batch['pixel_values'], pixel_mask=batch['pixel_mask'])

print("Shape of pixel_values:", pixel_values.shape)
print("Shape of logits:", outputs.logits.shape)
print("Shape of predicted bounding boxes:", outputs.pred_boxes.shape)
print("Shape of predicted masks:", outputs.pred_masks.shape)

from pytorch_lightning import Trainer

trainer = Trainer(gpus=1, max_epochs=25, gradient_clip_val=0.1)
trainer.fit(model)

# Commented out IPython magic to ensure Python compatibility.
! rm -r detr
! git clone https://github.com/facebookresearch/detr.git
# %cd detr

! pip install -q git+https://github.com/ade20kdataset/semanticapi.git

import torchvision
import os

class ade20kDetection(torchvision.datasets.ade20kDetection):
    def __init__(self, img_folder, ann_file, feature_extractor, train=True):
        super(ade20kDetection, self).__init__(img_folder, ann_file)
        self.feature_extractor = feature_extractor

    def __getitem__(self, idx):
        # read in PIL image and target in ade20k format
        img, target = super(ade20kDetection, self).__getitem__(idx)

        # preprocess image and target (converting target to DETR format, resizing + normalization of both image and target)
        image_id = self.ids[idx]
        target = {'image_id': image_id, 'annotations': target}
        encoding = self.feature_extractor(images=img, annotations=target, return_tensors="pt")
        pixel_values = encoding["pixel_values"].squeeze() # remove batch dimension
        target = encoding["labels"][0] # remove batch dimension

        return pixel_values, target

# load ground truths
base_ds = ade20kDetection(img_folder='/content/drive/MyDrive/DETR/ade20k data/',
                        ann_file='/content/drive/MyDrive/DETR/ade20k data/annotations/ade20k.json',
                        feature_extractor=feature_extractor, train=False).ade20k

from datasets.ade20k_eval import ade20kEvaluator

iou_types = ['bbox', 'segm']
ade20k_evaluator = ade20kEvaluator(base_ds, iou_types) # initialize evaluator with ground truths

"""Evaluation"""

import json

# read in all annotations
with open('/content/drive/MyDrive/DETR/ade20k data/annotations/ade20k_semantic/annotations/semantic_val2017.json') as f:
  data = json.load(f)

data.keys()

# get image ids of images in validation set
image_ids = []
for batch in val_dataloader:
  labels = batch['labels']
  for label in labels:
    image_ids.append(label['image_id'].item())
print(image_ids)

# only keep those annotations
relevant_annotations = []
for ann in data['annotations']:
  if ann['image_id'] in image_ids:
    relevant_annotations.append(ann)
print(len(relevant_annotations))

data['annotations'] = relevant_annotations

# write to json file
import json
with open('semantic_val.json', 'w') as f:
    json.dump(data, f)

# check whether this is ok
with open('semantic_val.json', 'r') as f:
        gt_json = json.load(f)
len(gt_json['annotations'])

from datasets.semantic_eval import semanticEvaluator

# inititialiaze semantic evaluator with the ground truth annotations
semantic_evaluator = semanticEvaluator(
            '/content/detr/semantic_val.json',
            val_dataloader.dataset.dataset.ann_folder,
            output_dir=".",
        )

from tqdm.notebook import tqdm

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

model.to(device)
model.eval()

print("Running evaluation...")

for idx, batch in enumerate(tqdm(val_dataloader)):
    # get the inputs
    pixel_values = batch["pixel_values"].to(device)
    pixel_mask = batch["pixel_mask"].to(device)
    labels = [{k: v.to(device) for k, v in t.items()} for t in batch["labels"]] # these are in DETR format, resized + normalized

    # forward pass
    with torch.no_grad():
      outputs = model.model(pixel_values=pixel_values, pixel_mask=pixel_mask)

    # object detection evaluation
    orig_target_sizes = torch.stack([target["orig_size"] for target in labels], dim=0)
    results = feature_extractor.post_process(outputs, orig_target_sizes) # convert outputs of model to ade20k api
    target_sizes = torch.stack([t["size"] for t in labels], dim=0)
    results = feature_extractor.post_process_segmentation(results, outputs, orig_target_sizes, target_sizes)
    res = {target['image_id'].item(): output for target, output in zip(labels, results)}
    ade20k_evaluator.update(res)

    # semantic segmentation evaluation
    target_sizes = torch.stack([t["size"] for t in labels], dim=0)
    results_semantic = feature_extractor.post_process_semantic(outputs, target_sizes, orig_target_sizes) # convert outputs of model to ade20k api
    for i, target in enumerate(labels):
        image_id = target["image_id"].item()
        file_name = f"{image_id:012d}.png"
        results_semantic[i]["image_id"] = image_id
        results_semantic[i]["file_name"] = file_name
    semantic_evaluator.update(results_semantic)

ade20k_evaluator.synchronize_between_processes()
ade20k_evaluator.accumulate()
ade20k_evaluator.summarize()

semantic_evaluator.synchronize_between_processes()
semantic_evaluator.summarize()

"""Inference"""

#We can use the image_id in target to know which image it is
pixel_values, target = val_dataset[1]

pixel_values = pixel_values.unsqueeze(0).to(device)
print(pixel_values.shape)

with torch.no_grad():
  # forward pass to get class logits and bounding boxes
  outputs = model(pixel_values=pixel_values, pixel_mask=None)

import torch

# use the post_process_semantic method of DetrFeatureExtractor, which expects as input the target size of the predictions (which we set here to the image size)
processed_sizes = torch.as_tensor(pixel_values.shape[-2:]).unsqueeze(0)
result = feature_extractor.post_process_semantic(outputs, processed_sizes)[0]

"""Compare to the original image and ground truth segmentation:"""

import itertools
import io
import seaborn as sns
import numpy
from transformers.models.detr.feature_extraction_detr import rgb_to_id
import matplotlib.pyplot as plt

palette = itertools.cycle(sns.color_palette())

# The segmentation is stored in a special-format png
semantic_seg = Image.open(io.BytesIO(result['png_string']))
semantic_seg = numpy.array(semantic_seg, dtype=numpy.uint8).copy()
# We retrieve the ids corresponding to each mask
semantic_seg_id = rgb_to_id(semantic_seg)

# Finally we color each mask individually
semantic_seg[:, :, :] = 0
for id in range(semantic_seg_id.max() + 1):
  semantic_seg[semantic_seg_id == id] = numpy.asarray(next(palette)) * 255
plt.figure(figsize=(15,15))
plt.imshow(semantic_seg)
plt.axis('off')
plt.show()

image_id = target['image_id'].item()
image = base_ds.loadImgs(image_id)[0]
img = Image.open(os.path.join('/content/drive/MyDrive/DETR/Cityscapes data/val2017', image['file_name']))
img

import matplotlib.pyplot as plt
import numpy as np

color_seg = np.zeros((predicted_segmentation_map.shape[0],
                      predicted_segmentation_map.shape[1], 3), dtype=np.uint8) # height, width, 3

palette = np.array(ade_palette())
for label, color in enumerate(palette):
    color_seg[predicted_segmentation_map == label, :] = color
color_seg = color_seg[..., ::-1]

img = np.array(image) * 0.5 + color_seg * 0.5
img = img.astype(np.uint8)

plt.figure(figsize=(15, 10))
plt.imshow(img)
plt.show()

map = Image.open('/content/ADE20k_toy_dataset/annotations/training/ADE_train_00000010.png')
map

# convert map to NumPy array
map = np.array(map)
map[map == 0] = 255 # background class is replaced by ignore_index
map = map - 1 # other classes are reduced by one
map[map == 254] = 255

classes_map = np.unique(map).tolist()
unique_classes = [model.config.id2label[idx] if idx!=255 else None for idx in classes_map]
print("Classes in this image:", unique_classes)

# create coloured map
color_seg = np.zeros((map.shape[0], map.shape[1], 3), dtype=np.uint8) # height, width, 3
palette = np.array(ade_palette())
for label, color in enumerate(palette):
    color_seg[map == label, :] = color
# Convert to BGR
color_seg = color_seg[..., ::-1]

# Show image + mask
img = np.array(image) * 0.5 + color_seg * 0.5
img = img.astype(np.uint8)

plt.figure(figsize=(15, 10))
plt.imshow(img)
plt.show()

import pandas as pd

# print overall metrics
for key in list(metrics.keys())[:3]:
  print(key, metrics[key])